# ğŸ§¹ DÃ©fi #21 : Le Nimbus 3000

> **Benchmark des Optimizers pour CNN Harry Potter**  
> Workshop Poudlard RP 2025 - Maison GÃ©miniard ğŸ¦…

---

## ğŸ“‹ Description du DÃ©fi

Benchmark complet des optimizers (Adadelta, Adam, etc.) sur un rÃ©seau de neurones existant, avec rÃ©daction d'un rapport au format papier de recherche.

**Points : 25 pts**

**Synergie avec DÃ©fi #20** : Utilise le CNN Harry Potter crÃ©Ã© prÃ©cÃ©demment

---

## ğŸ¯ Objectifs

1. âœ… Tester au moins 6 optimizers diffÃ©rents
2. âœ… Comparer leurs performances (accuracy, loss, temps)
3. âœ… GÃ©nÃ©rer des visualisations professionnelles
4. âœ… RÃ©diger un rapport de recherche complet

---

## ğŸ§ª Optimizers TestÃ©s (8 au total)

1. **Adam** - Adaptive Moment Estimation
2. **SGD** - Stochastic Gradient Descent (avec momentum)
3. **RMSprop** - Root Mean Square Propagation
4. **Adadelta** - Adaptive Delta
5. **Adagrad** - Adaptive Gradient
6. **Adamax** - Variante d'Adam avec norme infinie
7. **Nadam** - Nesterov Adam
8. **Ftrl** - Follow The Regularized Leader

---

## ğŸ—ï¸ MÃ©thodologie

### Dataset

- **Source** : CNN Harry Potter (DÃ©fi #20)
- **Classes** : 12 personnages
- **Images** : 100 par classe (1,200 total)
- **Split** : 80% train / 20% validation

### Configuration

| ParamÃ¨tre | Valeur |
|-----------|--------|
| Architecture | CNN Custom (4 blocs conv) |
| ParamÃ¨tres | ~3.5M |
| Batch Size | 32 |
| Epochs Max | 30 |
| Learning Rate | 0.001 (sauf Adadelta: 1.0) |
| Early Stopping | Oui (patience=5) |

### MÃ©triques ComparÃ©es

- **Accuracy** : PrÃ©cision finale et meilleure
- **Loss** : Perte de validation
- **Temps** : DurÃ©e d'entraÃ®nement
- **Convergence** : Epoch de meilleure accuracy
- **StabilitÃ©** : Variance dans les derniÃ¨res epochs

---

## ğŸš€ Utilisation

### Installation

```bash
cd defis/21_Nimbus_3000

# Installer les dÃ©pendances
pip install -r requirements.txt

# VÃ©rifier que le dÃ©fi #20 est complÃ©tÃ©
ls ../20_CNN_Harry/dataset  # Doit contenir les 12 dossiers de personnages
```

### ExÃ©cution du Benchmark

```bash
python optimizer_benchmark.py
```

**Temps estimÃ© :**
- CPU : ~3-4 heures (8 optimizers Ã— 30 epochs)
- GPU : ~40-60 minutes

### RÃ©sultats GÃ©nÃ©rÃ©s

Le script produit automatiquement :

```
defis/21_Nimbus_3000/
â”œâ”€â”€ benchmark_results.json       # RÃ©sultats bruts (JSON)
â”œâ”€â”€ benchmark_results.csv        # RÃ©sultats tabulaires (CSV)
â”œâ”€â”€ optimizer_comparison.png     # Courbes comparatives
â”œâ”€â”€ optimizer_heatmap.png        # Heatmap de performance
â”œâ”€â”€ optimizer_radar.png          # Radar chart multi-critÃ¨res
â”œâ”€â”€ research_paper.md            # Rapport de recherche complet
â””â”€â”€ summary.md                   # RÃ©sumÃ© exÃ©cutif
```

---

## ğŸ“Š Visualisations

### 1. Graphiques de Comparaison

**optimizer_comparison.png** contient 4 subplots :
- Validation Accuracy par optimizer
- Training Loss par optimizer  
- Temps d'entraÃ®nement (barres horizontales)
- Meilleure Accuracy (barres horizontales)

### 2. Heatmap de Performance

**optimizer_heatmap.png** montre :
- Best Accuracy (normalisÃ©e)
- Loss inversÃ©e (1 - loss)
- Vitesse normalisÃ©e

Couleurs : Vert = meilleur, Rouge = moins bon

### 3. Radar Chart

**optimizer_radar.png** compare sur 4 axes :
- **Accuracy** : Performance finale
- **Convergence** : RapiditÃ© Ã  atteindre le max
- **Vitesse** : Temps d'entraÃ®nement
- **StabilitÃ©** : Variance faible = stable

---

## ğŸ“ Rapport de Recherche

Le fichier `research_paper.md` contient :

### Structure

1. **RÃ©sumÃ© ExÃ©cutif**
   - Meilleur optimizer
   - Plus rapide
   - RÃ©sultats clÃ©s

2. **Introduction**
   - Contexte
   - Objectif

3. **MÃ©thodologie**
   - Dataset
   - Architecture CNN
   - Configuration expÃ©rimentale
   - Optimizers testÃ©s

4. **RÃ©sultats**
   - Tableau rÃ©capitulatif
   - Analyse par accuracy
   - Analyse par vitesse
   - Analyse de convergence

5. **Discussion**
   - Observations principales
   - Recommandations
   - Limitations

6. **Conclusion**
   - SynthÃ¨se
   - Meilleur choix

7. **RÃ©fÃ©rences**
   - Papers originaux

8. **Annexes**
   - Configuration systÃ¨me
   - ReproductibilitÃ©

---

## ğŸ“ˆ RÃ©sultats Attendus

### Classement Typique (Accuracy)

1. **Adam** / **Nadam** : ~92-95%
2. **RMSprop** : ~90-93%
3. **Adamax** : ~89-92%
4. **Adadelta** : ~87-90%
5. **Adagrad** : ~85-88%
6. **SGD** : ~83-87%
7. **Ftrl** : ~80-85%

*Note : RÃ©sultats peuvent varier selon le seed*

### Temps d'EntraÃ®nement

- **Plus rapides** : SGD, Ftrl (~200-300s)
- **Moyens** : Adam, RMSprop (~250-350s)
- **Plus lents** : Nadam, Adamax (~300-400s)

---

## ğŸ” Analyse des Optimizers

### Adam (Adaptive Moment Estimation)

**Avantages :**
- âœ… Converge rapidement
- âœ… Bon par dÃ©faut
- âœ… Adaptatif

**InconvÃ©nients :**
- âŒ Peut converger vers des minima moins gÃ©nÃ©ralisables

### SGD + Momentum

**Avantages :**
- âœ… Simple et Ã©prouvÃ©
- âœ… Bonne gÃ©nÃ©ralisation
- âœ… Rapide

**InconvÃ©nients :**
- âŒ Learning rate Ã  tuner
- âŒ Convergence plus lente

### RMSprop

**Avantages :**
- âœ… Bon pour RNN
- âœ… Adaptatif
- âœ… Stable

**InconvÃ©nients :**
- âŒ Learning rate Ã  choisir

### Adadelta

**Avantages :**
- âœ… Pas de learning rate Ã  dÃ©finir
- âœ… Adaptatif

**InconvÃ©nients :**
- âŒ Convergence parfois lente

### Nadam

**Avantages :**
- âœ… Combine Adam + Nesterov
- âœ… Meilleure convergence

**InconvÃ©nients :**
- âŒ LÃ©gÃ¨rement plus lent

---

## ğŸ“ InterprÃ©tation des RÃ©sultats

### Choisir selon le Cas d'Usage

**Pour Production (Accuracy Max) :**
â†’ Adam ou Nadam

**Pour Prototypage Rapide :**
â†’ Adam (bon compromis)

**Pour GÃ©nÃ©ralisation :**
â†’ SGD + Momentum

**Pour RNN/LSTM :**
â†’ RMSprop

**Sans Tuning de LR :**
â†’ Adadelta

---

## ğŸ”§ Personnalisation

### Ajouter un Optimizer

```python
# Dans optimizer_benchmark.py

OPTIMIZERS = {
    # ... existing optimizers
    'MonOptimizer': optimizers.MonOptimizer(learning_rate=0.001)
}
```

### Changer les HyperparamÃ¨tres

```python
# Learning rate diffÃ©rent
'Adam': optimizers.Adam(learning_rate=0.0001)

# SGD sans momentum
'SGD': optimizers.SGD(learning_rate=0.01, momentum=0.0)

# RMSprop avec decay
'RMSprop': optimizers.RMSprop(learning_rate=0.001, rho=0.9)
```

---

## ğŸ“ Structure du Projet

```
defis/21_Nimbus_3000/
â”œâ”€â”€ optimizer_benchmark.py       # Script principal (400+ lignes)
â”œâ”€â”€ requirements.txt             # DÃ©pendances
â”œâ”€â”€ README.md                    # Cette documentation
â”œâ”€â”€ prompts.md                   # Feuillet de prompts
â”‚
â”œâ”€â”€ benchmark_results.json       # RÃ©sultats bruts
â”œâ”€â”€ benchmark_results.csv        # RÃ©sultats CSV
â”‚
â”œâ”€â”€ optimizer_comparison.png     # Visualisation 1
â”œâ”€â”€ optimizer_heatmap.png        # Visualisation 2
â”œâ”€â”€ optimizer_radar.png          # Visualisation 3
â”‚
â”œâ”€â”€ research_paper.md            # Rapport complet (auto-gÃ©nÃ©rÃ©)
â””â”€â”€ summary.md                   # RÃ©sumÃ© (auto-gÃ©nÃ©rÃ©)
```

---

## ğŸ› Troubleshooting

### Erreur : Dataset non trouvÃ©

```bash
# CrÃ©er d'abord le dataset du dÃ©fi #20
cd ../20_CNN_Harry
python harry_potter_cnn.py
# Ou juste gÃ©nÃ©rer le dataset sans entraÃ®ner
```

### Erreur : Out of Memory

```python
# RÃ©duire le batch size dans optimizer_benchmark.py
BATCH_SIZE = 16  # au lieu de 32

# Ou rÃ©duire les epochs
EPOCHS = 20  # au lieu de 30
```

### Benchmark trop long

```python
# Tester moins d'optimizers
OPTIMIZERS = {
    'Adam': optimizers.Adam(learning_rate=LEARNING_RATE),
    'SGD': optimizers.SGD(learning_rate=LEARNING_RATE, momentum=0.9),
    'RMSprop': optimizers.RMSprop(learning_rate=LEARNING_RATE)
}
```

---

## âœ… Validation du DÃ©fi

**CritÃ¨res remplis :**

- [x] Benchmark sur rÃ©seau existant (CNN DÃ©fi #20)
- [x] Au moins 6 optimizers testÃ©s (8 au total)
- [x] Comparaison des performances
- [x] MÃ©triques multiples (accuracy, loss, temps, convergence)
- [x] Visualisations professionnelles (3 graphiques)
- [x] Rapport format papier de recherche (research_paper.md)
- [x] Documentation complÃ¨te
- [x] Code testable localement

**Points obtenus : 25 pts âœ…**

---

## ğŸ“š RÃ©fÃ©rences

### Papers Originaux

- **Adam**: Kingma & Ba (2014) - "Adam: A Method for Stochastic Optimization"
- **RMSprop**: Tieleman & Hinton (2012) - Lecture 6.5 of Neural Networks for Machine Learning
- **Adadelta**: Zeiler (2012) - "ADADELTA: An Adaptive Learning Rate Method"
- **Adagrad**: Duchi et al. (2011) - "Adaptive Subgradient Methods"
- **Nadam**: Dozat (2016) - "Incorporating Nesterov Momentum into Adam"

### Ressources

- [TensorFlow Optimizers](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)
- [Optimizer Comparison](https://ruder.io/optimizing-gradient-descent/)
- [Deep Learning Book - Ch 8](https://www.deeplearningbook.org/)

---

## ğŸ¯ AmÃ©liorations Possibles

1. **Grid Search** : Tester diffÃ©rents learning rates par optimizer
2. **Multiple Seeds** : Moyenne sur plusieurs runs
3. **Architectures** : Tester sur d'autres CNN
4. **Datasets** : Tester sur CIFAR-10, ImageNet
5. **Metrics** : Ajouter F1-score, Precision, Recall
6. **Visualizations** : Ajouter t-SNE, feature maps

---

## ğŸš€ Commandes Rapides

```bash
# Installation
pip install -r requirements.txt

# ExÃ©cution complÃ¨te
python optimizer_benchmark.py

# Voir les rÃ©sultats
cat summary.md
open optimizer_comparison.png  # macOS
xdg-open optimizer_comparison.png  # Linux
start optimizer_comparison.png  # Windows

# Lire le rapport
cat research_paper.md
```

---

**CrÃ©Ã© avec â¤ï¸ par l'Ã©quipe GÃ©miniard ğŸ¦…**  
**Workshop Poudlard RP 2025 - EPSI**

