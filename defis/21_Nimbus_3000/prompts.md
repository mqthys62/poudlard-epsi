# üìù Feuillet de Prompts - D√©fi #21 : Le Nimbus 3000

> **D√©fi Workshop Poudlard RP 2025**  
> **√âquipe :** G√©miniard ü¶Ö  
> **Date :** 13 octobre 2025  
> **Points :** 25 pts

---

## üéØ Contexte du D√©fi

**Objectif :** Benchmarker diff√©rents optimizers (Adadelta, Adam, etc.) sur le CNN Harry Potter (D√©fi #20), avec r√©daction d'un rapport au format papier de recherche.

**Exigences :**
- Tester au moins 6 optimizers
- Comparer performances (accuracy, loss, temps)
- G√©n√©rer des visualisations
- Rapport de recherche format acad√©mique

**Synergie :** Utilise le CNN du D√©fi #20

---

## üìã Prompts Utilis√©s

### Prompt 1 : Script de Benchmark Principal

```
Cr√©e un script Python complet pour benchmarker diff√©rents optimizers sur le CNN Harry Potter (D√©fi #20).

Contexte :
- R√©utiliser le CNN cr√©√© dans ../20_CNN_Harry/
- Tester 8 optimizers diff√©rents : Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl
- Comparer leurs performances sur les m√™mes donn√©es
- G√©n√©rer des visualisations professionnelles
- Produire un rapport de recherche

Script √† cr√©er : optimizer_benchmark.py

Structure :
1. Imports et configuration
2. Classe BenchmarkResults pour stocker les r√©sultats
3. Fonction prepare_data() : charge le dataset du d√©fi #20
4. Fonction benchmark_optimizer() : entra√Æne avec un optimizer
5. Fonction plot_comparison() : g√©n√®re 3 visualisations
6. Fonction generate_research_paper() : cr√©e le rapport MD
7. main() : orchestre tout

Configuration :
- IMG_SIZE = 224
- BATCH_SIZE = 32
- EPOCHS = 30 (r√©duit vs d√©fi #20 pour vitesse)
- LEARNING_RATE = 0.001 (sauf Adadelta = 1.0)

Optimizers avec TensorFlow :
OPTIMIZERS = {
    'Adam': optimizers.Adam(learning_rate=0.001),
    'SGD': optimizers.SGD(learning_rate=0.001, momentum=0.9),
    'RMSprop': optimizers.RMSprop(learning_rate=0.001),
    'Adadelta': optimizers.Adadelta(learning_rate=1.0),
    'Adagrad': optimizers.Adagrad(learning_rate=0.001),
    'Adamax': optimizers.Adamax(learning_rate=0.001),
    'Nadam': optimizers.Nadam(learning_rate=0.001),
    'Ftrl': optimizers.Ftrl(learning_rate=0.001)
}

M√©triques √† collecter par optimizer :
- History (loss, accuracy par epoch)
- Training time (en secondes)
- Final validation accuracy
- Final validation loss
- Best validation accuracy
- Epoch du best
- Nombre d'epochs avant early stopping

BenchmarkResults.to_dataframe() doit cr√©er un DataFrame pandas avec :
- Colonnes : Optimizer, Final Val Accuracy, Best Val Accuracy, Final Val Loss, Best Epoch, Training Time (s), Epochs Trained

Callbacks :
- EarlyStopping (monitor='val_loss', patience=5, restore_best_weights=True)

Sauvegardes :
- benchmark_results.json (d√©tails complets)
- benchmark_results.csv (tableau)

Importer depuis d√©fi #20 :
- from harry_potter_cnn import create_synthetic_dataset, build_cnn_model
- Ajouter sys.path.insert(0, '../20_CNN_Harry')
```

**R√©sultat :** Script optimizer_benchmark.py cr√©√© (400+ lignes)

---

### Prompt 2 : Visualisations Comparatives

```
Dans la fonction plot_comparison(benchmark_results), cr√©e 3 visualisations professionnelles.

Visualisation 1 : optimizer_comparison.png (4 subplots)
Figure size : (18, 10)

Subplot 1 (top-left) : Validation Accuracy
- Courbes des val_accuracy de tous les optimizers
- X-axis : Epoch, Y-axis : Accuracy
- L√©gendes, grid, titre

Subplot 2 (top-right) : Training Loss
- Courbes des training loss de tous les optimizers
- L√©gendes, grid, titre

Subplot 3 (bottom-left) : Temps d'Entra√Ænement
- Barres horizontales (plt.barh)
- Trier par temps croissant
- Afficher les valeurs en secondes √† droite des barres
- Couleur : skyblue

Subplot 4 (bottom-right) : Meilleure Accuracy
- Barres horizontales (plt.barh)
- Trier par accuracy d√©croissante
- Afficher les valeurs (format .4f) dans les barres
- Couleur : lightgreen
- xlim : [0, 1]

Visualisation 2 : optimizer_heatmap.png
Figure size : (12, 8)

Cr√©er une heatmap seaborn avec 3 colonnes :
- Best Accuracy (tel quel)
- Loss (inv) : 1 - final_val_loss (pour que plus = mieux)
- Vitesse (norm) : 1 / (train_time / 100) (normaliser)

Index : noms des optimizers
Colormap : 'RdYlGn' (rouge-jaune-vert)
Annotations : format '.3f'

Visualisation 3 : optimizer_radar.png
Figure size : (14, 14)
Projection : polar

4 crit√®res (axes) :
1. Accuracy : best_val_accuracy
2. Convergence : 1 - (best_epoch / EPOCHS) [plus t√¥t = mieux]
3. Vitesse : 1 - (train_time / max_train_time)
4. Stabilit√© : 1 - std(val_accuracy[-5:]) [moins de variance = mieux]

Pour chaque optimizer :
- Tracer un polygone avec plot() et fill()
- Alpha = 0.15 pour le fill
- L√©gendes

Sauvegarder toutes les figures en PNG (dpi=150, bbox_inches='tight')
```

**R√©sultat :** 3 visualisations PNG g√©n√©r√©es

---

### Prompt 3 : Rapport de Recherche Format Acad√©mique

```
Dans generate_research_paper(benchmark_results), cr√©e un rapport au format Markdown qui suit la structure d'un paper de recherche.

Structure obligatoire :

# Titre : "Le Nimbus 3000 : Benchmark des Optimizers pour CNN"

## R√©sum√© Ex√©cutif
- Contexte de l'√©tude
- Nombre d'optimizers test√©s
- Meilleur optimizer (nom + accuracy)
- Plus rapide (nom + temps)
- Dataset et architecture utilis√©s

## 1. Introduction
### 1.1 Contexte
- Importance des optimizers en Deep Learning
- Impact sur convergence, qualit√©, temps

### 1.2 Objectif
- Comparer empiriquement N optimizers
- Liste des optimizers test√©s

## 2. M√©thodologie
### 2.1 Dataset
- Source, classes, nombre d'images, r√©solution, split

### 2.2 Architecture CNN
- Diagramme texte de l'architecture
- Nombre de param√®tres
- Loss function et m√©triques

### 2.3 Configuration Exp√©rimentale
- Tableau avec : Batch Size, Epochs, LR, Early Stopping, Data Augmentation

### 2.4 Optimizers Test√©s
- Liste avec descriptions

## 3. R√©sultats
### 3.1 Tableau R√©capitulatif
- df.to_markdown(index=False) pour afficher le tableau

### 3.2 Analyse des Performances
#### 3.2.1 Accuracy
- Meilleur optimizer + valeur
- Classement par accuracy

#### 3.2.2 Vitesse d'Entra√Ænement
- Plus rapide + temps
- Classement par vitesse

#### 3.2.3 Convergence
- Meilleur convergence (epoch le plus bas)

### 3.3 Visualisations
- Liste des 3 images g√©n√©r√©es

## 4. Discussion
### 4.1 Observations Principales
- Comparaisons Adam vs SGD
- Observations sur adaptive LR
- Convergence et stabilit√©

### 4.2 Recommandations
- Pour production
- Pour dev rapide
- Compromis

### 4.3 Limitations
- Dataset synth√©tique
- Architecture fixe
- Hyperparam√®tres non optimis√©s
- Single seed

## 5. Conclusion
- Synth√®se des r√©sultats
- Meilleur choix global

## 6. R√©f√©rences
- Papers originaux (Adam, RMSprop, Adadelta, Adagrad)

## 7. Annexes
### 7.1 Configuration Syst√®me
- TensorFlow version
- Python version
- Hardware (GPU/CPU)

### 7.2 Reproductibilit√©
- Commande pour reproduire

Sauvegarder :
- research_paper.md (rapport complet)
- summary.md (r√©sum√© court avec meilleur optimizer)

Le rapport doit √™tre auto-g√©n√©r√© avec les vraies valeurs du benchmark.
Utiliser f-strings pour ins√©rer les r√©sultats dynamiquement.
```

**R√©sultat :** Rapport de recherche auto-g√©n√©r√© en Markdown

---

### Prompt 4 : README Documentation

```
Cr√©e un README.md complet pour le d√©fi #21 Le Nimbus 3000.

Structure :
1. En-t√™te (titre, description, points, synergie avec d√©fi #20)
2. Objectifs (liste avec checkmarks)
3. Optimizers test√©s (8 au total, avec descriptions br√®ves)
4. M√©thodologie (dataset, config, m√©triques)
5. Utilisation (installation, ex√©cution, temps estim√©)
6. R√©sultats g√©n√©r√©s (liste des fichiers produits)
7. Visualisations (description des 3 PNG)
8. Rapport de recherche (structure du MD)
9. R√©sultats attendus (classements typiques)
10. Analyse des optimizers (avantages/inconv√©nients de chacun)
11. Interpr√©tation (quand utiliser quel optimizer)
12. Personnalisation (comment ajouter un optimizer)
13. Structure du projet (arborescence)
14. Troubleshooting (dataset non trouv√©, OOM, benchmark long)
15. Validation du d√©fi (checklist)
16. R√©f√©rences (papers)
17. Am√©liorations possibles
18. Commandes rapides

Style :
- √âmojis pour les sections
- Tables pour donn√©es structur√©es
- Code blocks pour exemples
- Listes pour √©num√©rations
- Badges si applicable

Ton professionnel, adapt√© workshop √©tudiant.
Minimum 300 lignes.
```

**R√©sultat :** README.md (350+ lignes)

---

### Prompt 5 : Requirements.txt

```
Cr√©e requirements.txt avec les d√©pendances n√©cessaires pour le benchmark.

Packages requis :
- tensorflow (>=2.13.0) - pour les optimizers et le CNN
- numpy (>=1.24.0) - calculs
- pandas (>=2.0.0) - DataFrames pour r√©sultats
- matplotlib (>=3.7.0) - visualisations
- seaborn (>=0.12.0) - heatmap et styles
- tqdm (>=4.65.0) - barres de progression
- tabulate (>=0.9.0) - pour to_markdown() de pandas

Format : package>=version
Une d√©pendance par ligne
```

**R√©sultat :** requirements.txt avec 7 d√©pendances

---

## üìä R√©sultats Obtenus

### ‚úÖ Livrables Cr√©√©s

1. **Script de Benchmark** (`optimizer_benchmark.py`)
   - 400+ lignes
   - Classe BenchmarkResults
   - 8 optimizers test√©s
   - Sauvegarde JSON/CSV
   - G√©n√©ration automatique de visualisations et rapport

2. **Visualisations** (3 PNG)
   - `optimizer_comparison.png` - 4 subplots comparatifs
   - `optimizer_heatmap.png` - Heatmap performance
   - `optimizer_radar.png` - Radar chart multi-crit√®res

3. **Rapport de Recherche** (`research_paper.md`)
   - Format acad√©mique
   - 7 sections principales
   - Auto-g√©n√©r√© avec vraies donn√©es
   - Citations et r√©f√©rences

4. **Documentation** (`README.md`)
   - 350+ lignes
   - Guide complet d'utilisation
   - Analyse des optimizers
   - Troubleshooting

5. **Dependencies** (`requirements.txt`)
   - 7 packages
   - Versions sp√©cifi√©es

6. **Ce feuillet de prompts** (`prompts.md`)
   - 5 prompts d√©taill√©s
   - R√©sultats et apprentissages

### üìà Caract√©ristiques Techniques

**Benchmark :**
- **8 optimizers** test√©s (Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl)
- **30 epochs max** par optimizer
- **Early Stopping** (patience=5)
- **M√©triques** : Accuracy, Loss, Temps, Convergence, Stabilit√©

**Visualisations :**
- **4 graphiques** dans comparison (accuracy, loss, temps, best acc)
- **Heatmap** 3 crit√®res normalis√©s
- **Radar** 4 axes (accuracy, convergence, vitesse, stabilit√©)

**Rapport :**
- **Format acad√©mique** avec 7 sections
- **Auto-g√©n√©r√©** avec r√©sultats dynamiques
- **R√©f√©rences** aux papers originaux

### üéØ Validation

‚úÖ **D√©fi #21 : Le Nimbus 3000 - 25 points**

**Crit√®res valid√©s :**
- [x] Benchmark sur r√©seau existant (CNN D√©fi #20)
- [x] Au moins 6 optimizers (8 test√©s)
- [x] Comparaison de performances (5 m√©triques)
- [x] Visualisations professionnelles (3 PNG)
- [x] Rapport format papier de recherche (MD)
- [x] Documentation compl√®te (README)
- [x] Code testable localement
- [x] Reproductibilit√© garantie

---

## üîÑ It√©rations

### Version 1.0 (Initiale)
- 6 optimizers de base
- 2 visualisations simples
- Rapport basique

### Version 2.0 (Actuelle)
- **8 optimizers** (ajout Ftrl, Adamax)
- **3 visualisations** professionnelles
- **Radar chart** multi-crit√®res
- **Rapport acad√©mique** complet avec r√©f√©rences
- **Auto-g√©n√©ration** du rapport avec vraies donn√©es
- **Classe BenchmarkResults** pour structure

### Version 3.0 (Future - optionnelle)
- Grid search learning rates
- Multiple seeds (moyenne)
- Tests sur autres architectures
- Visualisations interactives (Plotly)

---

## üìù Notes Techniques

### Choix des Optimizers

**Pourquoi ces 8 ?**
1. **Adam** - Le plus populaire, baseline
2. **SGD** - Classique avec momentum
3. **RMSprop** - R√©f√©rence pour RNN
4. **Adadelta** - Pas de LR √† d√©finir
5. **Adagrad** - Historique, base d'Adam
6. **Adamax** - Variante Adam avec ‚àû-norm
7. **Nadam** - Adam + Nesterov
8. **Ftrl** - Sp√©cialis√© sparse features

**Exclus :**
- SGD sans momentum (moins performant)
- Optimizers obsol√®tes
- Optimizers exp√©rimentaux

### M√©triques Choisies

1. **Accuracy** : M√©trique principale
2. **Loss** : Qualit√© de l'optimisation
3. **Temps** : Co√ªt computationnel
4. **Convergence** : Efficacit√©
5. **Stabilit√©** : Robustesse

### Visualisations

**Pourquoi 3 types ?**
- **Comparison** : Vue temporelle (epochs)
- **Heatmap** : Vue globale normalis√©e
- **Radar** : Vue multi-crit√®res √©quilibr√©e

---

## üéì Apprentissages

### Prompt Engineering

- **R√©utilisation de code** : Import depuis d√©fi #20
- **Auto-g√©n√©ration** : Rapport avec f-strings dynamiques
- **Modularit√©** : Fonctions s√©par√©es et r√©utilisables
- **Visualisations vari√©es** : Subplots, heatmap, radar

### Machine Learning

- **Optimizers** : Diff√©rences significatives de performance
- **Hyperparam√®tres** : LR critique (Adadelta = 1.0)
- **Early Stopping** : Essentiel pour comparaison √©quitable
- **Normalisation** : N√©cessaire pour heatmap et radar

### Best Practices

- **Classe de r√©sultats** : Structure pour donn√©es
- **Sauvegarde multiple** : JSON + CSV + MD
- **Format acad√©mique** : Rapport professionnel
- **Reproductibilit√©** : Config compl√®te document√©e

---

## üöÄ Utilisation Rapide

```bash
# Installation
cd defis/21_Nimbus_3000
pip install -r requirements.txt

# V√©rifier dataset d√©fi #20
ls ../20_CNN_Harry/dataset

# Benchmark complet (3-4h CPU, 40-60min GPU)
python optimizer_benchmark.py

# R√©sultats
cat summary.md
cat research_paper.md
open optimizer_comparison.png
```

---

**Cr√©√© le :** 13 octobre 2025  
**√âquipe :** G√©miniard ü¶Ö  
**IA utilis√©e :** Gemini 2.5 Flash & Pro  
**Temps total :** ~1 heure

---

**Note :** Ce feuillet de prompts est une preuve de travail conforme aux exigences du Workshop Poudlard RP 2025. Tous les fichiers ont √©t√© g√©n√©r√©s par l'IA sans modification manuelle.

